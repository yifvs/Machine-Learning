import streamlit as st
import pandas as pd
from sklearn.decomposition import PCA
import plotly.figure_factory as ff
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, RobustScaler, MaxAbsScaler, Normalizer
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm
import numpy as np
import io
import matplotlib.pyplot as plt
import datetime
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib.font_manager as fm
import re
import plotly.express as px
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import shap

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨äºæ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨äºæ­£å¸¸æ˜¾ç¤ºè´Ÿå·

def evaluate_model(y_true, y_pred):
    """
    ç»Ÿä¸€æ¨¡å‹è¯„ä¼°å‡½æ•°
    è¿”å›åŒ…å«RMSEã€MAEã€RÂ²ã€MSEã€å‡†ç¡®ç‡çš„å­—å…¸
    """
    # è®¡ç®—å›å½’æŒ‡æ ‡
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ï¼ˆä»…å½“ç›®æ ‡å˜é‡ä¸ºåˆ†ç±»æ—¶ï¼‰
    try:
        acc = accuracy_score(y_true, np.round(y_pred))
    except ValueError:
        acc = None  # å¦‚æœç›®æ ‡å˜é‡æ˜¯è¿ç»­å€¼ï¼Œåˆ™è¿”å›None
        
    return {
        'RMSE': rmse,
        'MAE': mae,
        'RÂ²': r2,
        'MSE': mse,
        'Accuracy': acc
    }

# å®šä¹‰ TimeSeriesDataset ç±»
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y, sequence_length):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.X) - self.sequence_length

    def __getitem__(self, idx):
        return (
            self.X[idx:idx + self.sequence_length],
            self.y[idx + self.sequence_length - 1]
        )

# æ·»åŠ  LSTMModel ç±»å®šä¹‰
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        predictions = self.fc(lstm_out[:, -1, :])
        return predictions

# æ·»åŠ  GRUModel ç±»å®šä¹‰
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        gru_out, _ = self.gru(x)
        predictions = self.fc(gru_out[:, -1, :])
        return predictions
    
def shap_analysis(model, X_train, X_test, feature_columns):
    """
    ä½¿ç”¨ SHAP å€¼åˆ†ææ¨¡å‹çš„é¢„æµ‹ç»“æœ
    """
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©è§£é‡Šå™¨
    if isinstance(model, (xgb.XGBModel, RandomForestRegressor, lgb.LGBMModel)):
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_test)
    elif isinstance(model, torch.nn.Module):
        explainer = shap.DeepExplainer(model, torch.tensor(X_train, dtype=torch.float32))
        shap_values = explainer.shap_values(torch.tensor(X_test, dtype=torch.float32))
    elif isinstance(model, SVR):
        explainer = shap.LinearExplainer(model, X_train)
        shap_values = explainer.shap_values(X_test)
    else:
        explainer = shap.Explainer(model, X_train)
        shap_values = explainer(X_test)


    # å¯è§†åŒ–å…¨å±€ç‰¹å¾é‡è¦æ€§å›¾
    st.subheader("å…¨å±€ç‰¹å¾é‡è¦æ€§")
    shap_values_summary = np.mean(np.abs(shap_values), axis=0)
    importance_df = pd.DataFrame({
        'feature': feature_columns,
        'importance': shap_values_summary
    }).sort_values(by='importance', ascending=False)

    fig = px.bar(importance_df, x='importance', y='feature', orientation='h', title='å…¨å±€ç‰¹å¾é‡è¦æ€§')
    st.plotly_chart(fig)
def display_metrics(train_metrics, test_metrics):
    """
    æ˜¾ç¤ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„è¯„ä¼°æŒ‡æ ‡
    """
    # æ ¼å¼åŒ–å‡†ç¡®ç‡æ˜¾ç¤º
    train_acc = train_metrics['Accuracy'] if train_metrics['Accuracy'] is not None else 'N/A'
    test_acc = test_metrics['Accuracy'] if test_metrics['Accuracy'] is not None else 'N/A'

    # æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
    st.write(f"""
    ## æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
    ### è®­ç»ƒé›†
    - RMSE: {train_metrics['RMSE']:.4f}
    - MAE: {train_metrics['MAE']:.4f}
    - RÂ²: {train_metrics['RÂ²']:.4f}
    - MSE: {train_metrics['MSE']:.4f}
    - å‡†ç¡®ç‡: {train_acc if isinstance(train_acc, str) else f"{train_acc:.2%}"}

    ### æµ‹è¯•é›†  
    - RMSE: {test_metrics['RMSE']:.4f}
    - MAE: {test_metrics['MAE']:.4f}
    - RÂ²: {test_metrics['RÂ²']:.4f}
    - MSE: {test_metrics['MSE']:.4f}
    - å‡†ç¡®ç‡: {test_acc if isinstance(test_acc, str) else f"{test_acc:.2%}"}
    """)

# è®¾ç½®é¡µé¢å¸ƒå±€
st.set_page_config(layout="wide", page_title="Machine Learning", page_icon="ğŸ“ˆ")
# è®¾ç½®åº”ç”¨æ ‡é¢˜
st.title("æ•°æ®é¢„å¤„ç†ä¸æ¨¡å‹è®­ç»ƒ Web åº”ç”¨ V2.7")

# åˆ›å»ºä¸€ä¸ªè¾“å…¥æ¡†æ¥è·å–headerçš„å€¼
header = st.sidebar.text_input("è¯·è¾“å…¥æ•°æ®è¡¨ä¸­åˆ—åæ‰€åœ¨çš„è¡Œå·ï¼š:violet[(æ‰‹åŠ¨è¯‘ç æ•°æ®ä¸º0ï¼Œè‡ªåŠ¨è¯‘ç æ•°æ®ä¸º4)]", "4")

# åˆ›å»ºè¾“å…¥æ¡†æ¥è·å–è¦åˆ é™¤çš„è¡Œæ•°
num_rows_to_skip_before = st.sidebar.number_input("è¦è·³è¿‡çš„è¡Œæ•°ï¼ˆå‰ï¼‰ï¼š", min_value=0, value=0)
num_rows_to_skip_after = st.sidebar.number_input("è¦åˆ é™¤çš„è¡Œæ•°ï¼ˆåï¼‰ï¼š", min_value=0, value=0)

# å°†å¤„ç†Timeåˆ—ç©ºå€¼çš„å‡½æ•°å•ç‹¬æå–å‡ºæ¥
def handle_time_column_na(data):
    if 'Time' in data.columns and data['Time'].isnull().any():
        st.warning(f"Timeåˆ—ä¸­å­˜åœ¨ {data['Time'].isnull().sum()} ä¸ªç©ºå€¼")
        time_na_handling = st.selectbox(
            "è¯·é€‰æ‹©Timeåˆ—ç©ºå€¼å¤„ç†æ–¹æ³•",
            ["åˆ é™¤ç©ºå€¼è¡Œ", "å‰å‘å¡«å……", "åå‘å¡«å……"]
        )
        
        if time_na_handling == "åˆ é™¤ç©ºå€¼è¡Œ":
            data = data.dropna(subset=['Time'])
            st.info(f"å·²åˆ é™¤Timeåˆ—ä¸­çš„ç©ºå€¼è¡Œï¼Œå‰©ä½™ {len(data)} è¡Œæ•°æ®")
        elif time_na_handling == "å‰å‘å¡«å……":
            data['Time'].fillna(method='ffill', inplace=True)
            st.info("å·²ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……Timeåˆ—çš„ç©ºå€¼")
        elif time_na_handling == "åå‘å¡«å……":
            data['Time'].fillna(method='bfill', inplace=True)
            st.info("å·²ç”¨åä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……Timeåˆ—çš„ç©ºå€¼")
    return data

# ä¿®æ”¹ç¼“å­˜å‡½æ•°ï¼Œç§»é™¤widget
@st.cache_data
def load_data(files, header, num_rows_to_skip_before, num_rows_to_skip_after):
    data_list = []
    date_list = []

    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œç”¨äºåŒ¹é… YYYYMMDD æ ¼å¼
    date_pattern = re.compile(r'(\d{8})')

    for file in files:
        file_extension = file.name.split(".")[-1].lower()
        if file_extension == "csv":
            data = pd.read_csv(file, index_col=None, header=int(header), encoding='gb18030')
        elif file_extension == "xlsx":
            data = pd.read_excel(file, index_col=None, header=int(header))

        # åˆ é™¤å‰åæŒ‡å®šçš„è¡Œæ•°
        if num_rows_to_skip_before > 0:
            data = data.iloc[num_rows_to_skip_before:]
        if num_rows_to_skip_after > 0:
            data = data.iloc[:-num_rows_to_skip_after]

        # ä»…åœ¨ä¸Šä¼ å¤šä¸ªæ–‡ä»¶æ—¶æå–æ–‡ä»¶åä¸­çš„æ—¥æœŸä¿¡æ¯
        if len(files) > 1:
            match = date_pattern.search(file.name)
            if match:
                date_str = match.group(1)  # YYYYMMDD
                file_date = pd.to_datetime(date_str, format='%Y%m%d')
                date_list.append(file_date)
            else:
                date_list.append(pd.NaT)
        
        data_list.append(data)

    # åˆå¹¶å¤šä¸ªæ–‡ä»¶
    merged_data = pd.concat(data_list, axis=0)
    return merged_data

# å¯¼å…¥æ•°æ®
uploaded_files = st.file_uploader("ğŸ“ Please select the data files to import (multiple files can be uploaded):", type=["csv", "xlsx"],
                                  accept_multiple_files=True)
data = pd.DataFrame()  # åˆå§‹åŒ–ä¸ºç©ºçš„DataFrame

def process_time_column(data):
    try:
        if 'Time' in data.columns:
            # é¦–å…ˆæ£€æŸ¥ç¬¬ä¸€è¡Œæ•°æ®çš„æ ¼å¼
            sample_time = str(data['Time'].iloc[0])
            
            # å¦‚æœæ—¶é—´å­—ç¬¦ä¸²åŒ…å«æ—¥æœŸéƒ¨åˆ†
            if '/' in sample_time or '-' in sample_time:
                st.info("æ£€æµ‹åˆ°Timeåˆ—åŒ…å«æ—¥æœŸä¿¡æ¯ï¼Œæ­£åœ¨å¤„ç†...")
                # å°è¯•ç›´æ¥è½¬æ¢ä¸ºdatetime
                data['DateTime'] = pd.to_datetime(data['Time'], format='mixed')
                # ä¸éœ€è¦é¢å¤–çš„æ—¥æœŸå¤„ç†ï¼Œå› ä¸ºæ—¥æœŸä¿¡æ¯å·²ç»åŒ…å«åœ¨å†…
            else:
                # å¤„ç†çº¯æ—¶é—´æ ¼å¼ (HH:MM:SS)
                st.info("æ£€æµ‹åˆ°Timeåˆ—ä¸ºçº¯æ—¶é—´æ ¼å¼ï¼Œæ­£åœ¨å¤„ç†...")
                data['Time'] = pd.to_datetime(data['Time'], format='mixed').dt.time
                
                # æ£€æµ‹æ˜¯å¦å­˜åœ¨è·¨é›¶ç‚¹æƒ…å†µ
                times = pd.to_datetime(data['Time'].astype(str))
                time_diff = times.diff()
                
                # å¦‚æœå­˜åœ¨è´Ÿçš„æ—¶é—´å·®ï¼Œè¯´æ˜è·¨é›¶ç‚¹
                if (time_diff < pd.Timedelta(0)).any():
                    st.info("æ£€æµ‹åˆ°æ—¶é—´åºåˆ—è·¨é›¶ç‚¹ï¼Œæ­£åœ¨è¿›è¡Œæ—¥æœŸè°ƒæ•´...")
                    
                    # åˆå§‹åŒ–æ—¥æœŸåç§»
                    date_offset = pd.Timedelta(days=0)
                    new_dates = []
                    
                    # è·å–åŸºå‡†æ—¥æœŸ
                    if 'FileDate' in data.columns:
                        base_date = pd.to_datetime(data['FileDate'].iloc[0])
                    else:
                        base_date = pd.to_datetime('today')
                    
                    prev_time = None
                    for current_time in data['Time']:
                        if prev_time is not None:
                            # å¦‚æœå½“å‰æ—¶é—´å°äºå‰ä¸€ä¸ªæ—¶é—´ï¼Œè¯´æ˜è·¨é›¶ç‚¹
                            if current_time < prev_time:
                                date_offset += pd.Timedelta(days=1)
                        
                        # å°†æ—¶é—´å’Œæ—¥æœŸåç§»ç»„åˆ
                        new_datetime = base_date + date_offset + pd.Timedelta(
                            hours=current_time.hour,
                            minutes=current_time.minute,
                            seconds=current_time.second
                        )
                        new_dates.append(new_datetime)
                        prev_time = current_time
                    
                    data['DateTime'] = new_dates
                else:
                    # å¦‚æœæ²¡æœ‰è·¨é›¶ç‚¹ï¼Œæ­£å¸¸å¤„ç†
                    if 'FileDate' in data.columns:
                        data['DateTime'] = data.apply(
                            lambda row: pd.to_datetime(str(row['FileDate'])) + pd.Timedelta(
                                hours=row['Time'].hour,
                                minutes=row['Time'].minute,
                                seconds=row['Time'].second
                            ),
                            axis=1
                        )
                    else:
                        today = pd.to_datetime('today')
                        data['DateTime'] = data['Time'].apply(
                            lambda x: today + pd.Timedelta(
                                hours=x.hour,
                                minutes=x.minute,
                                seconds=x.second
                            )
                        )

            # è®¾ç½®DateTimeä¸ºç´¢å¼•
            if 'DateTime' in data.columns:
                data.set_index('DateTime', inplace=True)
                data.sort_index(inplace=True)
                st.success("æ—¶é—´åˆ—å¤„ç†æˆåŠŸï¼")
            
            return data
        else:
            st.warning("æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°Timeåˆ—")
            return data
            
    except Exception as e:
        st.error(f"å¤„ç†Timeåˆ—æ—¶å‡ºé”™ï¼š{str(e)}")
        st.error("è¯·æ£€æŸ¥Timeåˆ—çš„æ ¼å¼æ˜¯å¦ä¸€è‡´")
        return None
    
if uploaded_files is not None and len(uploaded_files) > 0:
    # æ•°æ®åŠ è½½
    data = load_data(uploaded_files, header, num_rows_to_skip_before, num_rows_to_skip_after)
    
    # å¤„ç†Timeåˆ—çš„ç©ºå€¼
    if data is not None and 'Time' in data.columns:
        # å¤„ç†ç©ºå€¼
        data = handle_time_column_na(data)
        # å¤„ç†æ—¶é—´æ ¼å¼
        data = process_time_column(data)

    st.success("æ•°æ®å·²æˆåŠŸå¯¼å…¥å¹¶åˆå¹¶ï¼")

    # æ˜¾ç¤ºè¡¨æ ¼æ•°æ®
    st.subheader("åŸå§‹è¡¨æ ¼æ•°æ®ï¼š")
    show_data = st.checkbox('æ˜¯å¦æ˜¾ç¤ºåŸå§‹è¡¨æ ¼æ•°æ®çš„å‰20è¡Œ', value=False)
    if show_data:
        st.dataframe(data.head(20))

    # æ•°æ®é¢„å¤„ç†éƒ¨åˆ†
    columns = st.sidebar.multiselect("é€‰æ‹©è¦é¢„å¤„ç†çš„åˆ—", data.columns.tolist())

    if columns:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«Timeåˆ—
        if 'Time' in columns:
            st.info("æ£€æµ‹åˆ°Timeåˆ—ï¼Œå°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸ºç´¢å¼•ï¼Œå¹¶ä¿æŒå…¶æ—¶é—´æ ¼å¼")
            try:
                # å¤„ç†Timeåˆ—ä¸­çš„ç©ºå€¼
                if data['Time'].isnull().any():
                    st.warning(f"Timeåˆ—ä¸­å­˜åœ¨ {data['Time'].isnull().sum()} ä¸ªç©ºå€¼")
                    time_na_handling = st.selectbox(
                        "è¯·é€‰æ‹©Timeåˆ—ç©ºå€¼å¤„ç†æ–¹æ³•",
                        ["åˆ é™¤ç©ºå€¼è¡Œ", "å‰å‘å¡«å……", "åå‘å¡«å……"]
                    )
                    
                    if time_na_handling == "åˆ é™¤ç©ºå€¼è¡Œ":
                        data = data.dropna(subset=['Time'])
                        st.info(f"å·²åˆ é™¤Timeåˆ—ä¸­çš„ç©ºå€¼è¡Œï¼Œå‰©ä½™ {len(data)} è¡Œæ•°æ®")
                    elif time_na_handling == "å‰å‘å¡«å……":
                        data['Time'].fillna(method='ffill', inplace=True)
                        st.info("å·²ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……Timeåˆ—çš„ç©ºå€¼")
                    elif time_na_handling == "åå‘å¡«å……":
                        data['Time'].fillna(method='bfill', inplace=True)
                        st.info("å·²ç”¨åä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……Timeåˆ—çš„ç©ºå€¼")

                # å°†Timeåˆ—è½¬æ¢ä¸ºdatetime.timeç±»å‹
                data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S').dt.time

                # å°†Timeåˆ—ä¸FileDateç»“åˆ
                if 'FileDate' in data.columns:
                    data['DateTime'] = data.apply(lambda row: datetime.datetime.combine(row['FileDate'], row['Time']), axis=1)
                    data.set_index('DateTime', inplace=True)
                    columns.remove('Time')
                    st.success("Timeåˆ—å·²æˆåŠŸè®¾ç½®ä¸ºç´¢å¼•")
                
                # æ˜¾ç¤ºæ—¶é—´èŒƒå›´ä¿¡æ¯
                time_range = data.index.max() - data.index.min()
                st.info(f"""
                æ•°æ®æ—¶é—´èŒƒå›´ä¿¡æ¯ï¼š
                - èµ·å§‹æ—¶é—´ï¼š{data.index.min().strftime('%Y-%m-%d %H:%M:%S')}
                - ç»“æŸæ—¶é—´ï¼š{data.index.max().strftime('%Y-%m-%d %H:%M:%S')}
                - æ€»æ—¶é•¿ï¼š{time_range}
                - æ•°æ®ç‚¹æ•°ï¼š{len(data)}
                """)
                
            except Exception as e:
                st.error(f"Timeåˆ—è½¬æ¢å¤±è´¥: {str(e)}")
                st.warning("Timeåˆ—å°†ä¿æŒåŸæ ¼å¼")
                columns.remove('Time')

        if columns:  # ç¡®ä¿è¿˜æœ‰å…¶ä»–åˆ—éœ€è¦å¤„ç†
            # æ•°æ®ç±»å‹è½¬æ¢
            convert_types = st.selectbox("é€‰æ‹©æ•°æ®ç±»å‹è½¬æ¢æ–¹å¼", ["ä¸è¿›è¡Œè½¬æ¢", "å­—ç¬¦ä¸²è½¬æ•°å€¼"])
            if convert_types == "å­—ç¬¦ä¸²è½¬æ•°å€¼":
                for col in columns:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
                st.write("ç±»å‹è½¬æ¢åçš„æ•°æ®é¢„è§ˆï¼š", data[columns].head(10))

            # ç©ºå€¼å¤„ç†
            missing_handling = st.selectbox("é€‰æ‹©ç©ºå€¼å¤„ç†æ–¹æ³•", ["ä¸å¤„ç†", "å‰å‘å¡«å……", "åå‘å¡«å……", "åˆ é™¤ç©ºå€¼", "çº¿æ€§æ’å€¼"])
            if missing_handling == "å‰å‘å¡«å……":
                data[columns] = data[columns].fillna(method='ffill')
            elif missing_handling == "åå‘å¡«å……":
                data[columns] = data[columns].fillna(method='bfill')
            elif missing_handling == "åˆ é™¤ç©ºå€¼":
                data = data.dropna(subset=columns)
            elif missing_handling == "çº¿æ€§æ’å€¼":
                # è¿›è¡Œçº¿æ€§æ’å€¼
                data[columns] = data[columns].interpolate(method='linear')
                # åˆ é™¤æ’å€¼åä»ç„¶å­˜åœ¨çš„ç©ºå€¼
                data = data.dropna(subset=columns)
            st.write("ç©ºå€¼å¤„ç†åçš„æ•°æ®é¢„è§ˆï¼š", data[columns].head(10))

            # å½’ä¸€åŒ–
            normalization = st.selectbox("é€‰æ‹©å½’ä¸€åŒ–æ–¹æ³•", ["ä¸è¿›è¡Œå½’ä¸€åŒ–", "æœ€å°-æœ€å¤§å½’ä¸€åŒ–", "Z-scoreæ ‡å‡†åŒ–", "æœ€å¤§ç»å¯¹å€¼å½’ä¸€åŒ–", "Robust Scaler", "L2å½’ä¸€åŒ–"])
            if normalization == "æœ€å°-æœ€å¤§å½’ä¸€åŒ–":
                scaler = MinMaxScaler()
                data[columns] = scaler.fit_transform(data[columns])
            elif normalization == "Z-scoreæ ‡å‡†åŒ–":
                scaler = StandardScaler()
                data[columns] = scaler.fit_transform(data[columns])
            elif normalization == "æœ€å¤§ç»å¯¹å€¼å½’ä¸€åŒ–":
                scaler = MaxAbsScaler()
                data[columns] = scaler.fit_transform(data[columns])
            elif normalization == "Robust Scaler":
                scaler = RobustScaler()
                data[columns] = scaler.fit_transform(data[columns])
            elif normalization == "L2å½’ä¸€åŒ–":
                scaler = Normalizer(norm='l2')
                data[columns] = scaler.fit_transform(data[columns])
            st.write("å½’ä¸€åŒ–åçš„æ•°æ®é¢„è§ˆï¼š", data[columns].head(10))

            # ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
            apply_pca = st.checkbox("æ˜¯å¦è¿›è¡Œä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰", value=False)
            if apply_pca:
                pca_columns = st.multiselect("é€‰æ‹©è¦è¿›è¡ŒPCAçš„ç‰¹å¾åˆ—", columns)
                if pca_columns:
                    n_components = st.number_input("é€‰æ‹©ä¸»æˆåˆ†æ•°é‡", min_value=1, max_value=len(pca_columns), value=2)
                    pca = PCA(n_components=n_components)
                    pca_result = pca.fit_transform(data[pca_columns])
                    pca_df = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(n_components)])
                    st.write("PCAç»“æœé¢„è§ˆï¼š", pca_df.head(10))

                    # è®¡ç®—å¹¶å±•ç¤ºç´¯ç§¯æ–¹å·®å›¾è¡¨
                    explained_variance_ratio = pca.explained_variance_ratio_
                    cumulative_variance_ratio = explained_variance_ratio.cumsum()

                    fig = px.line(
                        x=range(1, n_components + 1),
                        y=cumulative_variance_ratio,
                        labels={'x': 'ä¸»æˆåˆ†æ•°é‡', 'y': 'ç´¯ç§¯æ–¹å·®æ¯”ä¾‹'},
                        title='ç´¯ç§¯æ–¹å·®å›¾è¡¨'
                    )
                    fig.update_traces(mode='markers+lines')
                    st.plotly_chart(fig)

                    # è¾“å‡ºæ¯ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹
                    st.write("æ¯ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹ï¼š")
                    for i, variance_ratio in enumerate(explained_variance_ratio):
                        st.write(f"ä¸»æˆåˆ† {i+1}: {variance_ratio:.4f}")

                    # æä¾›ä¸‹è½½é€‰é¡¹
                    csv_buffer = io.BytesIO()
                    pca_df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
                    csv_buffer.seek(0)
                    st.download_button(
                        label="ä¸‹è½½PCAç»“æœ",
                        data=csv_buffer,
                        file_name="PCAç»“æœ.csv",
                        mime="text/csv"
                    )

            # é¢„å¤„ç†æ•°æ®ä¸‹è½½
            st.subheader("ä¸‹è½½é¢„å¤„ç†åçš„æ•°æ®")
            preprocessed_data = None

            if st.button("ç”Ÿæˆé¢„å¤„ç†æ•°æ®"):
                if columns:
                    # å¦‚æœTimeæ˜¯ç´¢å¼•ï¼Œå°†å…¶åŒ…å«åœ¨å¯¼å‡ºæ•°æ®ä¸­
                    if isinstance(data.index, pd.DatetimeIndex):
                        preprocessed_data = data[columns].copy()
                        preprocessed_data.index.name = 'Time'
                        preprocessed_data = preprocessed_data.reset_index()
                    else:
                        preprocessed_data = data[columns].copy()
                    st.success("é¢„å¤„ç†æ•°æ®å·²ç”Ÿæˆï¼Œå¯ä»¥ä¸‹è½½")
                else:
                    st.warning("è¯·å…ˆé€‰æ‹©è¦é¢„å¤„ç†çš„åˆ—")

            if preprocessed_data is not None:
                csv_buffer = io.BytesIO()
                preprocessed_data.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
                csv_buffer.seek(0)

                st.download_button(
                    label="ä¸‹è½½é¢„å¤„ç†æ•°æ®",
                    data=csv_buffer,
                    file_name="é¢„å¤„ç†æ•°æ®.csv",
                    mime="text/csv"
                )

        # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åºå¹¶è®¾ç½®æ—¶é—´ç´¢å¼•
        if 'DateTime' in data.columns:
            data.sort_values(by='DateTime', inplace=True)
            data.set_index('DateTime', inplace=True)
            st.success("æ•°æ®å·²æŒ‰DateTimeåˆ—æ’åºå¹¶è®¾ç½®ä¸ºç´¢å¼•")

        # ç¡®ä¿ç´¢å¼•æ˜¯DatetimeIndex
        if not isinstance(data.index, pd.DatetimeIndex):
            st.error("æ•°æ®ç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œè¯·æ£€æŸ¥æ•°æ®å¤„ç†æµç¨‹ã€‚")

        # æ—¶é—´åºåˆ—åˆ†è§£åˆ†æ
        st.subheader("æ—¶é—´åºåˆ—åˆ†è§£åˆ†æ")

        # é€‰æ‹©è¦åˆ†æçš„åˆ—
        decompose_column = st.selectbox(
            "é€‰æ‹©è¦è¿›è¡Œæ—¶é—´åºåˆ—åˆ†è§£çš„åˆ—",
            [""] + data.columns.tolist(),  # åœ¨åˆ—åå‰æ·»åŠ ä¸€ä¸ªç©ºå­—ç¬¦ä¸²
            index=0  # è®¾ç½®é»˜è®¤é€‰æ‹©ä¸ºç¬¬ä¸€ä¸ªé€‰é¡¹ï¼Œå³ç©ºå­—ç¬¦ä¸²
        )

        if decompose_column:
            # æ·»åŠ æ¨¡å‹é€‰æ‹©å’Œå‘¨æœŸè®¾ç½®
            col1, col2 = st.columns(2)
            with col1:
                model = st.selectbox(
                    "é€‰æ‹©åˆ†è§£æ¨¡å‹",
                    ["åŠ æ³•æ¨¡å‹", "ä¹˜æ³•æ¨¡å‹"]
                )
            with col2:
                period_type = st.selectbox(
                    "é€‰æ‹©å‘¨æœŸç±»å‹",
                    ["å°æ—¶", "å¤©", "å‘¨", "æœˆ"]
                )
                if period_type == "å°æ—¶":
                    period = st.number_input("è®¾ç½®å°æ—¶å‘¨æœŸ", min_value=1, value=24)
                elif period_type == "å¤©":
                    period = st.number_input("è®¾ç½®å¤©å‘¨æœŸ", min_value=1, value=7) * 24
                elif period_type == "å‘¨":
                    period = st.number_input("è®¾ç½®å‘¨å‘¨æœŸ", min_value=1, value=1) * 24 * 7
                else:  # æœˆ
                    period = st.number_input("è®¾ç½®æœˆå‘¨æœŸ", min_value=1, value=1) * 24 * 30

            # æ·»åŠ ç”Ÿæˆå›¾å½¢çš„æŒ‰é’®
            if st.button("ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†è§£å›¾"):
                try:
                    # è·å–æ•°æ®å¹¶ç¡®ä¿æŒ‰æ—¶é—´æ’åº
                    ts_data = data[decompose_column].sort_index()

                    # æ’å…¥ NaN æ¥è¡¨ç¤ºæ—¶é—´åºåˆ—ä¸­çš„æ–­ç‚¹
                    ts_data = ts_data.asfreq('T')  # å‡è®¾æ•°æ®æ˜¯æŒ‰åˆ†é’Ÿé¢‘ç‡é‡‡æ ·çš„
                    ts_data = ts_data.interpolate(method='time', limit_direction='both')

                    # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
                    st.info(f"""
                    æ•°æ®æ—¶é—´èŒƒå›´ï¼š
                    å¼€å§‹æ—¶é—´ï¼š{ts_data.index.min()}
                    ç»“æŸæ—¶é—´ï¼š{ts_data.index.max()}
                    æ•°æ®ç‚¹æ•°é‡ï¼š{len(ts_data)}
                    """)

                    # è¿›è¡Œæ—¶é—´åºåˆ—åˆ†è§£
                    decomposition = seasonal_decompose(
                        ts_data,
                        period=int(period),
                        model='additive' if model == "åŠ æ³•æ¨¡å‹" else 'multiplicative'
                    )

                    # åˆ›å»ºå›¾è¡¨ï¼Œå¢åŠ å‚ç›´é—´è·
                    fig = make_subplots(
                        rows=4, cols=1,
                        subplot_titles=('åŸå§‹æ•°æ®', 'è¶‹åŠ¿', 'å­£èŠ‚æ€§', 'æ®‹å·®'),
                        vertical_spacing=0.15,
                        shared_xaxes=True
                    )

                    # æ·»åŠ åŸå§‹æ•°æ®ï¼Œè®¾ç½® connectgaps=False ä»¥ä¸è¿æ¥ç©ºç™½å¤„
                    fig.add_trace(
                        go.Scatter(
                            x=ts_data.index, 
                            y=ts_data.values, 
                            name='åŸå§‹æ•°æ®',
                            connectgaps=False,  # ä¸è¿æ¥ç©ºç™½å¤„
                            mode='lines',  # ä½¿ç”¨çº¿æ¡æ¨¡å¼
                        ),
                        row=1, col=1
                    )

                    # æ·»åŠ è¶‹åŠ¿
                    fig.add_trace(
                        go.Scatter(
                            x=ts_data.index, 
                            y=decomposition.trend, 
                            name='è¶‹åŠ¿',
                            connectgaps=False,
                            mode='lines',
                        ),
                        row=2, col=1
                    )

                    # æ·»åŠ å­£èŠ‚æ€§
                    fig.add_trace(
                        go.Scatter(
                            x=ts_data.index, 
                            y=decomposition.seasonal, 
                            name='å­£èŠ‚æ€§',
                            connectgaps=False,
                            mode='lines',
                        ),
                        row=3, col=1
                    )

                    # æ·»åŠ æ®‹å·®
                    fig.add_trace(
                        go.Scatter(
                            x=ts_data.index, 
                            y=decomposition.resid, 
                            name='æ®‹å·®',
                            connectgaps=False,
                            mode='lines',
                        ),
                        row=4, col=1
                    )

                    # æ›´æ–°å¸ƒå±€ï¼Œä¼˜åŒ–æ ‡ç­¾å’Œé—´è·
                    fig.update_layout(
                        height=800,
                        showlegend=False,
                        margin=dict(t=60, b=20, l=80, r=20),
                    )

                    # åªä¸ºæœ€åº•éƒ¨çš„å­å›¾æ˜¾ç¤ºxè½´æ ‡ç­¾
                    fig.update_xaxes(title_text="", showticklabels=False, row=1)
                    fig.update_xaxes(title_text="", showticklabels=False, row=2)
                    fig.update_xaxes(title_text="", showticklabels=False, row=3)
                    fig.update_xaxes(title_text="æ—¶é—´", row=4)

                    # æ›´æ–°æ¯ä¸ªå­å›¾çš„ Y è½´æ ‡é¢˜ï¼Œè°ƒæ•´ä½ç½®
                    fig.update_yaxes(title_text="å€¼", title_standoff=5, row=1, col=1)
                    fig.update_yaxes(title_text="è¶‹åŠ¿", title_standoff=5, row=2, col=1)
                    fig.update_yaxes(title_text="å­£èŠ‚æ€§", title_standoff=5, row=3, col=1)
                    fig.update_yaxes(title_text="æ®‹å·®", title_standoff=5, row=4, col=1)

                    # æ›´æ–°å­å›¾æ ‡é¢˜çš„å­—ä½“å’Œä½ç½®
                    for i in range(len(fig.layout.annotations)):
                        fig.layout.annotations[i].update(
                            y=fig.layout.annotations[i].y + 0.03,  # å‘ä¸Šç§»åŠ¨å­å›¾æ ‡é¢˜
                            font=dict(size=12)  # è°ƒæ•´å­—ä½“å¤§å°
                        )

                    # æ˜¾ç¤ºå›¾è¡¨
                    st.plotly_chart(fig, use_container_width=True)

                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    st.subheader("åˆ†è§£ç»“æœç»Ÿè®¡ä¿¡æ¯")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.write("è¶‹åŠ¿ç»Ÿè®¡")
                        st.write(decomposition.trend.describe())
                    
                    with col2:
                        st.write("å­£èŠ‚æ€§ç»Ÿè®¡")
                        st.write(decomposition.seasonal.describe())
                    
                    with col3:
                        st.write("æ®‹å·®ç»Ÿè®¡")
                        st.write(decomposition.resid.describe())
                    
                    # æä¾›ä¸‹è½½åˆ†è§£ç»“æœçš„åŠŸèƒ½
                    decomp_df = pd.DataFrame({
                        'åŸå§‹æ•°æ®': ts_data,
                        'è¶‹åŠ¿': decomposition.trend,
                        'å­£èŠ‚æ€§': decomposition.seasonal,
                        'æ®‹å·®': decomposition.resid
                    })
                    
                    csv_buffer = io.BytesIO()
                    decomp_df.to_csv(csv_buffer, index=True, encoding="utf-8-sig")
                    csv_buffer.seek(0)
                    
                    st.download_button(
                        label="ä¸‹è½½åˆ†è§£ç»“æœ",
                        data=csv_buffer,
                        file_name=f"{decompose_column}_æ—¶é—´åºåˆ—åˆ†è§£.csv",
                        mime="text/csv"
                    )
                    
                    # æ·»åŠ è§£é‡Šæ€§è¯´æ˜
                    st.info("""
                    æ—¶é—´åºåˆ—åˆ†è§£è¯´æ˜ï¼š
                    
                    1. è¶‹åŠ¿ï¼ˆTrendï¼‰ï¼šåæ˜ æ•°æ®çš„é•¿æœŸå˜åŒ–æ–¹å‘
                    2. å­£èŠ‚æ€§ï¼ˆSeasonalï¼‰ï¼šåæ˜ æ•°æ®çš„å‘¨æœŸæ€§å˜åŒ–è§„å¾‹
                    3. æ®‹å·®ï¼ˆResidualï¼‰ï¼šå»é™¤è¶‹åŠ¿å’Œå­£èŠ‚æ€§åçš„éšæœºæ³¢åŠ¨
                    
                    åŠ æ³•æ¨¡å‹ï¼šåŸå§‹æ•°æ® = è¶‹åŠ¿ + å­£èŠ‚æ€§ + æ®‹å·®
                    ä¹˜æ³•æ¨¡å‹ï¼šåŸå§‹æ•°æ® = è¶‹åŠ¿ Ã— å­£èŠ‚æ€§ Ã— æ®‹å·®
                    """)
                    
                    # æ·»åŠ å‘¨æœŸæ€§åˆ†æç»“æœ
                    st.subheader("å‘¨æœŸæ€§åˆ†æ")
                    seasonal_pattern = decomposition.seasonal[:period]
                    
                    # åˆ›å»ºå‘¨æœŸæ¨¡å¼å›¾
                    fig_pattern = go.Figure()
                    
                    # åˆ›å»ºæ—¶é—´ç‚¹åˆ—è¡¨
                    time_points = np.arange(int(period))
                    
                    # æ·»åŠ å‘¨æœŸæ¨¡å¼ï¼Œç¡®ä¿xå’Œyçš„æ•°æ®ç±»å‹ä¸€è‡´
                    fig_pattern.add_trace(go.Scatter(
                        x=time_points,  # ä½¿ç”¨numpyæ•°ç»„
                        y=np.array(seasonal_pattern),  # è½¬æ¢ä¸ºnumpyæ•°ç»„
                        mode='lines+markers',
                        name='å­£èŠ‚æ€§æ¨¡å¼'
                    ))
                    
                    # æ›´æ–°å¸ƒå±€
                    fig_pattern.update_layout(
                        title=f'{period_type}å‘¨æœŸæ¨¡å¼',
                        xaxis_title=f'å‘¨æœŸå†…æ—¶é—´ç‚¹ï¼ˆæ€»è®¡{period}å°æ—¶ï¼‰',
                        yaxis_title='å­£èŠ‚æ€§æˆåˆ†å€¼',
                        showlegend=True
                    )
                    
                    st.plotly_chart(fig_pattern, use_container_width=True)
                    
                    # æ·»åŠ å‘¨æœŸç‰¹å¾ç»Ÿè®¡
                    st.write("å‘¨æœŸç‰¹å¾ç»Ÿè®¡ï¼š")
                    st.write({
                        "å‘¨æœŸé•¿åº¦": f"{period}å°æ—¶",
                        "å‘¨æœŸå³°å€¼": f"{seasonal_pattern.max():.3f}",
                        "å‘¨æœŸè°·å€¼": f"{seasonal_pattern.min():.3f}",
                        "å‘¨æœŸæ³¢åŠ¨èŒƒå›´": f"{seasonal_pattern.max() - seasonal_pattern.min():.3f}"
                    })

                except Exception as e:
                    st.error(f"åˆ†è§£è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
                    st.warning("è¯·ç¡®ä¿æ•°æ®å·²æ­£ç¡®è®¾ç½®æ—¶é—´ç´¢å¼•ï¼Œä¸”é€‰æ‹©äº†åˆé€‚çš„åˆ†è§£å‘¨æœŸã€‚")
        
        # ç›¸å…³æ€§åˆ†æ
        st.subheader("ç›¸å…³æ€§åˆ†æ")
        
        # é€‰æ‹©ç›®æ ‡å˜é‡
        target_var = st.selectbox(
            "é€‰æ‹©ç›®æ ‡å˜é‡",
            [""] + data.columns.tolist()
        )
        
        if target_var:
            # é€‰æ‹©ç‰¹å¾å˜é‡ï¼ˆå¤šé€‰ï¼‰
            feature_vars = st.multiselect(
                "é€‰æ‹©ç‰¹å¾å˜é‡ï¼ˆå¯å¤šé€‰ï¼‰",
                [col for col in data.columns if col != target_var]
            )
            
            if feature_vars:
                # é€‰æ‹©ç›¸å…³ç³»æ•°ç±»å‹
                corr_method = st.radio(
                    "é€‰æ‹©ç›¸å…³ç³»æ•°ç±»å‹",
                    ["Pearson", "Spearman"],
                    horizontal=True
                )
                
                # æ•°æ®é¢„å¤„ç†
                corr_data = data[[target_var] + feature_vars].copy()
                
                # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                for col in corr_data.columns:
                    corr_data[col] = pd.to_numeric(corr_data[col], errors='coerce')
                
                # ä½¿ç”¨çº¿æ€§æ’å€¼å¡«å……ç©ºå€¼
                corr_data = corr_data.interpolate(method='linear')
                # å¤„ç†é¦–å°¾çš„ç©ºå€¼
                corr_data = corr_data.fillna(method='bfill').fillna(method='ffill')
                
                if st.button("ç”Ÿæˆç›¸å…³æ€§çƒ­å›¾"):
                    # è®¡ç®—ç›¸å…³ç³»æ•°
                    corr_matrix = corr_data.corr(method=corr_method.lower())

                    # åˆ›å»ºçƒ­å›¾
                    fig = ff.create_annotated_heatmap(
                        z=corr_matrix.values,
                        x=corr_matrix.columns.tolist(),
                        y=corr_matrix.index.tolist(),
                        colorscale='Viridis',
                        showscale=True,
                        annotation_text=corr_matrix.round(2).values,
                        hoverinfo="z"
                    )

                    fig.update_layout(
                        title=f'{corr_method} Correlation Matrix',
                        xaxis=dict(tickmode='array', tickvals=list(range(len(corr_matrix.columns))), ticktext=corr_matrix.columns),
                        yaxis=dict(tickmode='array', tickvals=list(range(len(corr_matrix.index))), ticktext=corr_matrix.index),
                        margin=dict(l=100, r=100, t=100, b=100)
                    )

                    # æ˜¾ç¤ºå›¾å½¢
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ˜¾ç¤ºä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§æ’åº
                    st.write(f"ä¸{target_var}çš„{corr_method}ç›¸å…³ç³»æ•°æ’åºï¼š")
                    
                    # è·å–ç›®æ ‡å˜é‡çš„ç›¸å…³ç³»æ•°å¹¶æ’åº
                    target_corr = corr_matrix[target_var].drop(target_var)
                    target_corr_sorted = target_corr.abs().sort_values(ascending=False)
                    
                    # åˆ›å»ºä¸€ä¸ªDataFrameæ¥æ˜¾ç¤ºç›¸å…³æ€§æ’åº
                    corr_df = pd.DataFrame({
                        'å˜é‡': target_corr_sorted.index,
                        'ç›¸å…³ç³»æ•°': target_corr[target_corr_sorted.index],
                        'ç»å¯¹å€¼': target_corr_sorted.values
                    })
                    
                    # è®¾ç½®æ˜¾ç¤ºæ ¼å¼
                    pd.set_option('display.float_format', lambda x: '%.3f' % x)
                    
                    # æ˜¾ç¤ºç»“æœ
                    st.dataframe(corr_df)
                    
                    # æ·»åŠ è§£é‡Šæ€§æ–‡æœ¬
                    st.info("""
                    ç›¸å…³ç³»æ•°è§£é‡Šï¼š
                    - å–å€¼èŒƒå›´ï¼š-1 åˆ° 1
                    - 1ï¼šå®Œå…¨æ­£ç›¸å…³
                    - -1ï¼šå®Œå…¨è´Ÿç›¸å…³
                    - 0ï¼šæ— ç›¸å…³æ€§
                    - ç»å¯¹å€¼è¶Šå¤§è¡¨ç¤ºç›¸å…³æ€§è¶Šå¼º
                    """)

        # é¢‘è°±åˆ†æéƒ¨åˆ†
        st.subheader("é¢‘è°±åˆ†æ")

        # é€‰æ‹©è¦è¿›è¡Œé¢‘è°±åˆ†æçš„åˆ—
        spectrum_columns = st.multiselect("é€‰æ‹©è¦è¿›è¡Œé¢‘è°±åˆ†æçš„åˆ—", data.columns.tolist(), key='spectrum_cols')

        # é€‰æ‹©ç­›é€‰æ¡ä»¶çš„åˆ—ï¼ˆå¯å¤šé€‰ï¼‰
        filter_columns = st.multiselect(
            "é€‰æ‹©ç”¨äºç­›é€‰æ•°æ®èŒƒå›´çš„åˆ—ï¼ˆå¯å¤šé€‰ï¼‰",
            [col for col in data.columns if col not in spectrum_columns]  # æ’é™¤å·²é€‰æ‹©çš„åˆ†æåˆ—
        )

        filter_conditions = []
        filter_ranges = {}

        if filter_columns:
            st.write("è®¾ç½®æ•°æ®ç­›é€‰èŒƒå›´ï¼š")
            for col in filter_columns:
                min_val = float(data[col].min())
                max_val = float(data[col].max())
                
                range_min, range_max = st.slider(
                    f"é€‰æ‹© {col} çš„èŒƒå›´",
                    min_value=min_val,
                    max_value=max_val,
                    value=(min_val, max_val),
                    step=(max_val - min_val) / 100
                )
                
                filter_ranges[col] = (range_min, range_max)
                filter_conditions.append(
                    (data[col] >= range_min) & (data[col] <= range_max)
                )

        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if filter_columns and filter_conditions:
            final_filter = filter_conditions[0]
            for condition in filter_conditions[1:]:
                final_filter = final_filter & condition
            filtered_data = data[final_filter][spectrum_columns]
        else:
            filtered_data = data[spectrum_columns]

        if spectrum_columns:
            analysis_type = st.selectbox("é€‰æ‹©åˆ†ææ–¹æ³•", ["å¿«é€Ÿå‚…é‡Œå¶å˜æ¢(FFT)", "è¿ç»­å°æ³¢å˜æ¢(CWT)"])
            
            if analysis_type == "å¿«é€Ÿå‚…é‡Œå¶å˜æ¢(FFT)":
                if st.button("ç”ŸæˆFFTé¢‘è°±å›¾"):
                    # Calculate layout
                    n_cols = min(2, len(spectrum_columns))
                    n_rows = (len(spectrum_columns) + n_cols - 1) // n_cols
                    
                    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 3*n_rows))
                    if len(spectrum_columns) == 1:
                        axes = np.array([[axes]])
                    elif n_rows == 1:
                        axes = axes.reshape(1, -1)
                    
                    for idx, col in enumerate(spectrum_columns):
                        row = idx // n_cols
                        col_idx = idx % n_cols
                        
                        signal = filtered_data[col].values
                        n = len(signal)
                        fft_result = np.fft.fft(signal)
                        freq = np.fft.fftfreq(n, d=1)
                        
                        axes[row, col_idx].plot(freq[:n//2], np.abs(fft_result)[:n//2])
                        axes[row, col_idx].set_title(f'FFT of {col}')
                        axes[row, col_idx].set_xlabel('Frequency (Hz)')
                        axes[row, col_idx].set_ylabel('Magnitude')
                        axes[row, col_idx].grid(True)
                    
                    # Handle extra subplots
                    for idx in range(len(spectrum_columns), n_rows * n_cols):
                        row = idx // n_cols
                        col_idx = idx % n_cols
                        fig.delaxes(axes[row, col_idx])
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.close()
            
            else:  # CWTåˆ†æ
                if st.button("ç”ŸæˆCWTæ—¶é¢‘å›¾"):
                    import pywt
                    scales = np.arange(1, 128)
                    wavelet = 'morl'
                    
                    for col in spectrum_columns:
                        signal = filtered_data[col].values
                        coefficients, frequencies = pywt.cwt(signal, scales, wavelet)
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            # 2D Time-Frequency plot
                            fig, ax = plt.subplots(figsize=(5, 3))
                            im = ax.pcolormesh(np.arange(len(signal)), frequencies, np.abs(coefficients))
                            ax.set_title(f'CWT of {col}')
                            ax.set_ylabel('Frequency (Hz)')
                            ax.set_xlabel('Time (s)')
                            cbar = plt.colorbar(im, ax=ax, label='Magnitude')
                            # è°ƒæ•´colorbarçš„å­—ä½“å¤§å°
                            cbar.ax.tick_params(labelsize=8)
                            # è°ƒæ•´ä¸»å›¾çš„å­—ä½“å¤§å°
                            ax.tick_params(labelsize=8)
                            plt.tight_layout()
                            st.pyplot(fig)
                            plt.close()
                        
                        with col2:
                            # 3D Time-Frequency plot
                            fig = plt.figure(figsize=(5, 3))
                            ax = fig.add_subplot(111, projection='3d')
                            time_grid, scale_grid = np.meshgrid(np.arange(len(signal)), frequencies)
                            surf = ax.plot_surface(time_grid, scale_grid, np.abs(coefficients), cmap='viridis', linewidth=0)
                            ax.set_title(f'3D CWT of {col}', pad=2)  # å‡å°æ ‡é¢˜å’Œå›¾ä¹‹é—´çš„é—´è·
                            ax.set_xlabel('Time (s)', labelpad=2)    # å‡å°æ ‡ç­¾å’Œè½´ä¹‹é—´çš„é—´è·
                            ax.set_ylabel('Frequency (Hz)', labelpad=2)
                            ax.set_zlabel('Magnitude', labelpad=2)
                            # è°ƒæ•´è§†è§’
                            ax.view_init(elev=30, azim=45)
                            # è°ƒæ•´è½´æ ‡ç­¾å­—ä½“å¤§å°
                            ax.tick_params(labelsize=8)
                            cbar = plt.colorbar(surf, ax=ax, label='Magnitude', pad=0.1)  # å‡å°colorbarçš„é—´è·
                            cbar.ax.tick_params(labelsize=8)
                            plt.tight_layout()
                            st.pyplot(fig)
                            plt.close()

        # æ¦‚ç‡å¯†åº¦åˆ†å¸ƒåˆ†æéƒ¨åˆ†
        st.subheader("æ¦‚ç‡å¯†åº¦åˆ†å¸ƒåˆ†æ")
        
        # å¤šé€‰
        density_columns = st.multiselect(
            "é€‰æ‹©è¦åˆ†ææ¦‚ç‡å¯†åº¦åˆ†å¸ƒçš„åˆ—ï¼ˆå¯å¤šé€‰ï¼‰",
            data.columns.tolist(),
            key='density_cols'  # æ·»åŠ å”¯ä¸€çš„key
        )
        
        if density_columns:
            # é€‰æ‹©ç­›é€‰æ¡ä»¶çš„åˆ—ï¼ˆå¯å¤šé€‰ï¼‰
            filter_columns = st.multiselect(
                "é€‰æ‹©ç”¨äºç­›é€‰æ•°æ®èŒƒå›´çš„åˆ—ï¼ˆå¯å¤šé€‰ï¼‰",
                [col for col in data.columns if col not in density_columns],  # æ’é™¤å·²é€‰æ‹©çš„åˆ†æåˆ—
                key='filter_cols'  # æ·»åŠ å”¯ä¸€çš„key
            )
            
            if filter_columns:
                # æ•°æ®é¢„å¤„ç†
                processed_data = data.copy()
                
                for col in filter_columns:
                    processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce')
                    processed_data[col] = processed_data[col].interpolate(method='linear')
                    processed_data[col] = processed_data[col].fillna(method='bfill').fillna(method='ffill')
                
                st.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼š\n" + 
                       "\n".join([f"{col}: {processed_data[col].isna().sum()} ä¸ªç©ºå€¼" 
                                 for col in filter_columns]))
                
                filter_conditions = []
                filter_ranges = {}
                
                st.write("è®¾ç½®æ•°æ®ç­›é€‰èŒƒå›´ï¼š")
                for col in filter_columns:
                    min_val = float(processed_data[col].min())
                    max_val = float(processed_data[col].max())
                    
                    range_min, range_max = st.slider(
                        f"é€‰æ‹© {col} çš„èŒƒå›´",
                        min_value=min_val,
                        max_value=max_val,
                        value=(min_val, max_val),
                        step=(max_val - min_val) / 100,
                        key=f"slider_{col}"  # ä¸ºæ¯ä¸ªslideræ·»åŠ å”¯ä¸€çš„key
                    )
                    
                    filter_ranges[col] = (range_min, range_max)
                    filter_conditions.append(
                        (processed_data[col] >= range_min) & (processed_data[col] <= range_max)
                    )
            
            if st.button("ç”Ÿæˆæ¦‚ç‡å¯†åº¦åˆ†å¸ƒå›¾"):
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if filter_columns and filter_conditions:
                    final_filter = filter_conditions[0]
                    for condition in filter_conditions[1:]:
                        final_filter = final_filter & condition
                    filtered_data = processed_data[final_filter][density_columns]
                else:
                    filtered_data = data[density_columns]

                # æ£€æŸ¥filtered_dataæ˜¯å¦ä¸ºç©ºæˆ–density_columnsæ˜¯å¦ä¸ºç©º
                if filtered_data.empty or not density_columns:
                    st.warning("ç­›é€‰åçš„æ•°æ®ä¸ºç©ºæˆ–æœªé€‰æ‹©ä»»ä½•åˆ—ï¼Œè¯·è°ƒæ•´ç­›é€‰æ¡ä»¶ã€‚")
                else:
                    # åˆ›å»ºä¸€ä¸ªå›¾è¡¨æ¥æ˜¾ç¤ºæ‰€æœ‰é€‰ä¸­åˆ—çš„æ¦‚ç‡å¯†åº¦åˆ†å¸ƒ
                    fig = go.Figure()
                    
                    # ä¸ºæ¯ä¸ªé€‰çš„åˆ—æ·»åŠ ç›´æ–¹å›¾å’Œå¯†åº¦æ›²çº¿
                    for column in density_columns:
                        if column in filtered_data.columns:
                            # æ·»åŠ ç›´æ–¹å›¾
                            fig.add_trace(go.Histogram(
                                x=filtered_data[column],
                                name=f'{column} - ç›´æ–¹å›¾',
                                histnorm='probability density',
                                opacity=0.5,
                                nbinsx=50,
                                showlegend=True
                            ))
                            
                            # æ·»åŠ æ ¸å¯†åº¦ä¼°è®¡æ›²çº¿
                            from scipy import stats
                            kde = stats.gaussian_kde(filtered_data[column].dropna())
                            x_range = np.linspace(filtered_data[column].min(), filtered_data[column].max(), 200)
                            fig.add_trace(go.Scatter(
                                x=x_range,
                                y=kde(x_range),
                                name=f'{column} - å¯†åº¦æ›²çº¿',
                                line=dict(width=2)
                            ))
                    
                    # æ›´æ–°å¸ƒå±€
                    fig.update_layout(
                        title="å¤šå‚æ•°æ¦‚ç‡å¯†åº¦åˆ†å¸ƒå¯¹æ¯”å›¾",
                        xaxis_title="å€¼",
                        yaxis_title="å¯†åº¦",
                        showlegend=True,
                        legend=dict(
                            yanchor="top",
                            y=0.99,
                            xanchor="right",
                            x=0.99
                        ),
                        bargap=0.1,
                        barmode='overlay'  # ä½¿ç›´æ–¹å›¾é‡å æ˜¾ç¤º
                    )
                    
                    # æ˜¾ç¤ºå›¾è¡¨
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ˜¾ç¤ºæ‰€æœ‰å‚æ•°çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
                    st.write("**æ‰€æœ‰å‚æ•°çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼š**")
                    stats_df = pd.DataFrame({
                        'ç»Ÿè®¡é‡': ['å‡å€¼', 'ä¸­ä½æ•°', 'æ ‡å‡†å·®', 'æœ€å°å€¼', 'æœ€å¤§å€¼']
                    })
                    
                    for column in density_columns:
                        if column in filtered_data.columns:
                            stats_df[column] = [
                                f"{filtered_data[column].mean():.3f}",
                                f"{filtered_data[column].median():.3f}",
                                f"{filtered_data[column].std():.3f}",
                                f"{filtered_data[column].min():.3f}",
                                f"{filtered_data[column].max():.3f}"
                            ]
                    
                    st.write(stats_df)

        # é¦–å…ˆå®šä¹‰è¾…åŠ©å‡½æ•°
        def generate_future_features(X, future_dates, feature_columns):
            """
            æ ¹æ®å†å²æ•°æ®ç”Ÿæˆæœªæ¥ç‰¹å¾
            è¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç‰¹å¾çš„æ€§è´¨æ¥å®ç°å…·ä½“çš„ç”Ÿæˆé€»è¾‘
            """
            # ç¤ºä¾‹å®ç°ï¼šä½¿ç”¨æœ€åä¸€å¤©çš„ç‰¹å¾å€¼é‡å¤
            last_features = X.iloc[-1]
            future_features = pd.DataFrame([last_features.values] * len(future_dates),
                                         index=future_dates,
                                         columns=feature_columns)
            return future_features

        # æ¨¡å‹è®­ç»ƒéƒ¨åˆ†
        st.subheader("æ¨¡å‹è®­ç»ƒ")
        
        # æ·»åŠ ç©ºé€‰é¡¹ä½œä¸ºé»˜è®¤å€¼
        target_options = [""] + data.columns.tolist()
        target_column = st.selectbox("é€‰æ‹©ç›®æ ‡åˆ—ï¼ˆé¢„æµ‹ç›®æ ‡ï¼‰ï¼š", target_options)
        
        # åªæœ‰å½“é€‰æ‹©äº†ç›®æ ‡åˆ—æ—¶æ‰æ˜¾ç¤ºåç»­é€‰é¡¹
        if target_column:
            feature_columns = st.multiselect("é€‰æ‹©ç‰¹å¾åˆ—ï¼š", [col for col in data.columns if col != target_column])

            model_choice = st.selectbox("é€‰æ‹©æ¨¡å‹", ["çº¿æ€§å›å½’", "å¤šé¡¹å¼å›å½’", "ARIMA", "æ¢¯åº¦ä¸‹é™", "LSTM", "GRU", "XGBoost", "RandomForest", "LightGBM", "SVR"])

            # æ·»åŠ æ¨¡å‹å‚æ•°è®¾ç½®
            if model_choice == "å¤šé¡¹å¼å›å½’":
                degree = st.number_input("å¤šé¡¹å¼å›å½’çš„é˜¶æ•°", min_value=2, max_value=5, value=2)
            elif model_choice == "ARIMA":
                p = st.number_input("ARIMAçš„på‚æ•°", min_value=0, max_value=5, value=1)
                d = st.number_input("ARIMAçš„då‚æ•°", min_value=0, max_value=5, value=1)
                q = st.number_input("ARIMAçš„qå‚æ•°", min_value=0, max_value=5, value=1)
            elif model_choice == "æ¢¯åº¦ä¸‹é™":
                learning_rate = st.slider("é€‰æ‹©å­¦ä¹ ç‡", min_value=0.0001, max_value=1.0, value=0.01)
                max_iter = st.number_input("æœ€å¤§è¿­ä»£æ¬¡æ•°", min_value=100, max_value=10000, value=1000)
                tol = st.number_input("å®¹å¿åº¦", min_value=1e-3, max_value=1.0, value=1e-2)
                random_state = st.number_input("éšæœºç§å­", min_value=0, max_value=1000, value=42)
            elif model_choice == "LSTM":
                # LSTMå‚æ•°è®¾ç½®
                hidden_size = st.number_input("éšè—å±‚å¤§å°", min_value=32, max_value=256, value=64)
                num_layers = st.number_input("LSTMå±‚æ•°", min_value=1, max_value=3, value=1)
                epochs = st.number_input("è®­ç»ƒè½®æ•°", min_value=10, max_value=500, value=100)
                batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=16, max_value=128, value=32)
                sequence_length = st.number_input("åºåˆ—é•¿åº¦", min_value=1, max_value=48, value=24)
                learning_rate = st.slider("å­¦ä¹ ç‡", min_value=0.00001, max_value=0.01, value=0.001, step=0.0001, format="%.5f")
            elif model_choice == "GRU":
                # GRUå‚æ•°è®¾ç½®
                hidden_size = st.number_input("éšè—å±‚å¤§å°", min_value=32, max_value=256, value=64)
                num_layers = st.number_input("GRUå±‚æ•°", min_value=1, max_value=3, value=1)
                epochs = st.number_input("è®­ç»ƒè½®æ•°", min_value=10, max_value=500, value=100)
                batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=16, max_value=128, value=32)
                sequence_length = st.number_input("åºåˆ—é•¿åº¦", min_value=1, max_value=48, value=24)
                learning_rate = st.slider("å­¦ä¹ ç‡", min_value=0.00001, max_value=0.01, value=0.001, step=0.0001, format="%.5f")
            elif model_choice == "XGBoost":
                n_estimators = st.number_input("æ ‘çš„æ•°é‡", min_value=100, max_value=1000, value=100)
                max_depth = st.number_input("æ ‘çš„æœ€å¤§æ·±åº¦", min_value=1, max_value=10, value=3)
                learning_rate = st.slider("å­¦ä¹ ç‡", min_value=0.01, max_value=0.3, value=0.1)
                subsample = st.slider("å­æ ·æœ¬æ¯”ä¾‹", min_value=0.5, max_value=1.0, value=1.0)
                colsample_bytree = st.slider("æ¯æ£µæ ‘ä½¿ç”¨çš„ç‰¹å¾æ¯”ä¾‹", min_value=0.5, max_value=1.0, value=1.0)
                reg_lambda = st.number_input("L2æ­£åˆ™åŒ–é¡¹", min_value=0.0, max_value=10.0, value=1.0)
                reg_alpha = st.number_input("L1æ­£åˆ™åŒ–é¡¹", min_value=0.0, max_value=10.0, value=0.0)
            elif model_choice == "RandomForest":
                n_estimators = st.number_input("æ ‘çš„æ•°é‡", min_value=100, max_value=1000, value=100)
                max_depth = st.number_input("æ ‘çš„æœ€å¤§æ·±åº¦", min_value=1, max_value=30, value=10)
                min_samples_split = st.number_input("å†…éƒ¨èŠ‚ç‚¹å†åˆ’åˆ†æ‰€éœ€æœ€å°æ ·æœ¬æ•°", min_value=2, max_value=10, value=2)
                min_samples_leaf = st.number_input("å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°", min_value=1, max_value=10, value=1)
                max_features = st.selectbox("æœ€å¤§ç‰¹å¾æ•°", ["æŒ‡å®šæµ®ç‚¹æ•°å€¼", "sqrt", "log2"])
                if max_features == "æŒ‡å®šæµ®ç‚¹æ•°å€¼":
                    max_features = st.number_input("è¾“å…¥æœ€å¤§ç‰¹å¾æ•°çš„æ¯”ä¾‹ (0.0, 1.0]", min_value=0.0, max_value=1.0, value=0.5)
                else:
                    max_features = max_features
            elif model_choice == "LightGBM":
                n_estimators = st.number_input("æ ‘çš„æ•°é‡", min_value=100, max_value=1000, value=100)
                max_depth = st.number_input("æ ‘çš„æœ€å¤§æ·±åº¦", min_value=-1, max_value=50, value=-1)
                learning_rate = st.slider("å­¦ä¹ ç‡", min_value=0.01, max_value=0.3, value=0.1)
                num_leaves = st.number_input("å¶å­èŠ‚ç‚¹æ•°", min_value=2, max_value=256, value=31)
                min_child_samples = st.number_input("å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°", min_value=1, max_value=100, value=20)
                metric = st.selectbox("é€‰æ‹©è¯„ä¼°æŒ‡æ ‡", ["l2", "l1", "huber", "fair", "poisson", "quantile", "mape", "gamma", "tweedie"])
                boosting_type = st.selectbox("é€‰æ‹©æå‡ç±»å‹", ["gbdt", "dart", "goss", "rf"])
            elif model_choice == "SVR":
                kernel = st.selectbox("é€‰æ‹©æ ¸å‡½æ•°", ["linear", "poly", "rbf", "sigmoid"])
                C = st.number_input("æ­£åˆ™åŒ–å‚æ•°C", min_value=0.1, max_value=100.0, value=1.0)
                epsilon = st.number_input("epsilonå‚æ•°", min_value=0.01, max_value=1.0, value=0.1)

            # æ·»åŠ æœªæ¥é¢„æµ‹å¤©æ•°é€‰æ‹©
            future_days = st.number_input("é¢„æµ‹æœªæ¥å¤©æ•°", min_value=1, max_value=30, value=1)

            if st.button("å¼€å§‹è®­ç»ƒæ¨¡å‹"):
                try:
                    X = data[feature_columns]
                    y = data[target_column]
        
                    # æŒ‰æ—¶é—´é¡ºåºæ’åº
                    data = data.sort_index()
                    X = X.sort_index()
                    y = y.sort_index()
        
                    # ä¿æŒç´¢å¼•çš„åˆ‡åˆ†
                    train_size = int(len(data) * 0.8)
                    train_index = data.index[:train_size]
                    test_index = data.index[train_size:]
                    
                    X_train = X.loc[train_index]
                    X_test = X.loc[test_index]
                    y_train = y.loc[train_index]
                    y_test = y.loc[test_index]
        
                    # ç”Ÿæˆæœªæ¥æ—¶é—´ç‚¹
                    last_date = data.index[-1]
                    future_dates = pd.date_range(start=last_date, periods=future_days*24+1, freq='H')[1:]
        
                    # è®­ç»ƒæ¨¡å‹å¹¶é¢„æµ‹
                    if model_choice == "çº¿æ€§å›å½’":
                        model = LinearRegression()
                        model.fit(X_train, y_train)
                        y_train_pred = pd.Series(model.predict(X_train), index=train_index)
                        y_test_pred = pd.Series(model.predict(X_test), index=test_index)
                        
                        # ç”Ÿæˆæœªæ¥é¢„æµ‹
                        future_X = generate_future_features(X, future_dates, feature_columns)
                        future_pred = pd.Series(model.predict(future_X), index=future_dates)
        
                    elif model_choice == "å¤šé¡¹å¼å›å½’":
                        poly = PolynomialFeatures(degree=degree)
                        X_poly_train = poly.fit_transform(X_train)
                        X_poly_test = poly.transform(X_test)
                        model = LinearRegression()
                        model.fit(X_poly_train, y_train)
                        y_train_pred = pd.Series(model.predict(X_poly_train), index=train_index)
                        y_test_pred = pd.Series(model.predict(X_poly_test), index=test_index)
                        
                        # ç”Ÿæˆæœªæ¥é¢„æµ‹
                        future_X = generate_future_features(X, future_dates, feature_columns)
                        future_X_poly = poly.transform(future_X)
                        future_pred = pd.Series(model.predict(future_X_poly), index=future_dates)
        
                    elif model_choice == "ARIMA":
                        # æ£€æŸ¥ç‰¹å¾å‚æ•°æ•°é‡
                        if len(feature_columns) > 1:
                            st.warning("ARIMAæ¨¡å‹åªèƒ½å¤„ç†å•å˜é‡æ—¶é—´åºåˆ—æ•°æ®ï¼Œè¯·ç¡®ä¿ç‰¹å¾åˆ—æ•°é‡ä¸º1ã€‚")
                        
                        # ç¡®ä¿ç´¢å¼•æ˜¯æ—¶é—´åºåˆ—
                        y_train = y_train.asfreq('H')
                        y_test = y_test.asfreq('H')
        
                        model = sm.tsa.ARIMA(y_train, order=(p, d, q))
                        model_fit = model.fit()
        
                        # è·å–è®­ç»ƒé›†é¢„æµ‹å€¼
                        y_train_pred = pd.Series(model_fit.predict(start=train_index[0], end=train_index[-1]), index=train_index)
        
                        # è·å–æµ‹è¯•é›†é¢„æµ‹å€¼
                        y_test_pred = pd.Series(model_fit.predict(start=test_index[0], end=test_index[-1]), index=test_index)
        
                        # é¢„æµ‹æœªæ¥å€¼
                        future_steps = len(future_dates)
                        future_pred = pd.Series(model_fit.forecast(steps=future_steps), index=future_dates)
        
                    elif model_choice == "æ¢¯åº¦ä¸‹é™":
                        model = SGDRegressor(
                            learning_rate='constant',
                            eta0=learning_rate,
                            max_iter=max_iter,
                            tol=tol,
                            random_state=random_state
                        )
                        model.fit(X_train, y_train)
                        y_train_pred = pd.Series(model.predict(X_train), index=train_index)
                        y_test_pred = pd.Series(model.predict(X_test), index=test_index)
                        
                        # ç”Ÿæˆæœªæ¥é¢„æµ‹
                        future_X = generate_future_features(X, future_dates, feature_columns)
                        future_pred = pd.Series(model.predict(future_X), index=future_dates)

                    elif model_choice == "LSTM":
                        # æ•°æ®æ ‡å‡†åŒ–
                        scaler_X = StandardScaler()
                        scaler_y = StandardScaler()
                        
                        X_scaled = scaler_X.fit_transform(X)
                        y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))
                        
                        # å‡†å¤‡è®­ç»ƒé›†å’Œæµ‹è¯•é›†
                        train_data = TimeSeriesDataset(
                            X_scaled[:train_size], 
                            y_scaled[:train_size], 
                            sequence_length
                        )
                        test_data = TimeSeriesDataset(
                            X_scaled[train_size:], 
                            y_scaled[train_size:], 
                            sequence_length
                        )
                        
                        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)
                        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
                        
                        # åˆå§‹åŒ–æ¨¡å‹
                        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                        model = LSTMModel(len(feature_columns), hidden_size, num_layers).to(device)
                        criterion = nn.MSELoss()
                        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

                        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨æ¯ä¸ªepochçš„è®­ç»ƒæŸå¤±
                        train_losses = []
                        
                        # è®­ç»ƒæ¨¡å‹
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        for epoch in range(epochs):
                            model.train()
                            running_loss = 0.0
                            for batch_X, batch_y in train_loader:
                                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                                optimizer.zero_grad()
                                outputs = model(batch_X)
                                loss = criterion(outputs, batch_y)
                                loss.backward()
                                optimizer.step()

                                running_loss += loss.item()

                            # è®¡ç®—å¹¶è®°å½•æ¯ä¸ªepochçš„å¹³å‡è®­ç»ƒæŸå¤±
                            epoch_loss = running_loss / len(train_loader)
                            train_losses.append(epoch_loss)
                            
                            progress_bar.progress((epoch + 1) / epochs)
                            status_text.text(f'è®­ç»ƒè¿›åº¦: {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')

                        # ç»˜åˆ¶è®­ç»ƒæŸå¤±å›¾
                        fig_loss = go.Figure()
                        fig_loss.add_trace(go.Scatter(x=np.arange(1, epochs+1), y=train_losses, mode='lines+markers'))
                        fig_loss.update_layout(title='è®­ç»ƒæŸå¤±æ›²çº¿', xaxis_title='Epoch', yaxis_title='Loss')
                        st.plotly_chart(fig_loss, use_container_width=True)
                        status_text.text('è®­ç»ƒå®Œæˆ')
                        progress_bar.empty()
                        st.write('')

                        
                        # ç”Ÿæˆé¢„æµ‹
                        model.eval()
                        with torch.no_grad():
                            # è®­ç»ƒé›†é¢„æµ‹
                            train_predictions = []
                            for batch_X, _ in train_loader:
                                batch_X = batch_X.to(device)
                                outputs = model(batch_X)
                                train_predictions.extend(outputs.cpu().numpy())
                            
                            # æµ‹è¯•é›†é¢„æµ‹
                            test_predictions = []
                            for batch_X, _ in test_loader:
                                batch_X = batch_X.to(device)
                                outputs = model(batch_X)
                                test_predictions.extend(outputs.cpu().numpy())
                            
                            # æœªæ¥é¢„æµ‹
                            future_X = generate_future_features(X, future_dates, feature_columns)
                            future_X_scaled = scaler_X.transform(future_X)
                            
                            # å‡†å¤‡æœ€åä¸€ä¸ªåºåˆ—ç”¨äºé¢„æµ‹
                            last_sequence = torch.FloatTensor(X_scaled[-sequence_length:]).unsqueeze(0).to(device)
                            future_predictions = []
                            
                            # é€æ­¥é¢„æµ‹æœªæ¥å€¼
                            for _ in range(len(future_dates)):
                                next_pred = model(last_sequence)
                                future_predictions.append(next_pred.item())
                                # æ›´æ–°åºåˆ—
                                last_sequence = torch.cat([
                                    last_sequence[:, 1:, :],
                                    torch.FloatTensor(future_X_scaled[_:_+1]).unsqueeze(0).to(device)
                                ], dim=1)
                            
                            # è½¬æ¢å›åŸå§‹æ¯”ä¾‹
                            y_train_pred = pd.Series(scaler_y.inverse_transform(np.array(train_predictions).reshape(-1, 1)).flatten(),index=train_index[sequence_length:])
                            y_test_pred = pd.Series(scaler_y.inverse_transform(np.array(test_predictions).reshape(-1, 1)).flatten(),index=test_index[sequence_length:])
                            future_pred = pd.Series(scaler_y.inverse_transform(np.array(future_predictions).reshape(-1, 1)).flatten(),index=future_dates)
                            
                            # è°ƒæ•´è¯„ä¼°æ•°æ®çš„èŒƒå›´
                            y_train = y_train.iloc[sequence_length:]
                            y_test = y_test.iloc[sequence_length:]
        
                            # ä½¿ç”¨ç»Ÿä¸€è¯„ä¼°å‡½æ•°è®¡ç®—æŒ‡æ ‡
                            train_metrics = evaluate_model(y_train, y_train_pred)
                            test_metrics = evaluate_model(y_test, y_test_pred)

                            # è°ƒç”¨ display_metrics å‡½æ•°æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
                            display_metrics(train_metrics, test_metrics)

                    elif model_choice == "GRU":
                        # æ•°æ®æ ‡å‡†åŒ–
                        scaler_X = StandardScaler()
                        scaler_y = StandardScaler()
                        
                        X_scaled = scaler_X.fit_transform(X)
                        y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))
                        
                        # å‡†å¤‡è®­ç»ƒé›†å’Œæµ‹è¯•é›†
                        train_data = TimeSeriesDataset(
                            X_scaled[:train_size], 
                            y_scaled[:train_size], 
                            sequence_length
                        )
                        test_data = TimeSeriesDataset(
                            X_scaled[train_size:], 
                            y_scaled[train_size:], 
                            sequence_length
                        )
                        
                        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)
                        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
                        
                        # åˆå§‹åŒ–æ¨¡å‹
                        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                        model = GRUModel(len(feature_columns), hidden_size, num_layers).to(device)
                        criterion = nn.MSELoss()
                        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

                        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨æ¯ä¸ªepochçš„è®­ç»ƒæŸå¤±
                        train_losses = []
                        
                        # è®­ç»ƒæ¨¡å‹
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        for epoch in range(epochs):
                            model.train()
                            running_loss = 0.0
                            for batch_X, batch_y in train_loader:
                                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                                optimizer.zero_grad()
                                outputs = model(batch_X)
                                loss = criterion(outputs, batch_y)
                                loss.backward()
                                optimizer.step()

                                running_loss += loss.item()

                            # è®¡ç®—å¹¶è®°å½•æ¯ä¸ªepochçš„å¹³å‡è®­ç»ƒæŸå¤±
                            epoch_loss = running_loss / len(train_loader)
                            train_losses.append(epoch_loss)
                            
                            progress_bar.progress((epoch + 1) / epochs)
                            status_text.text(f'è®­ç»ƒè¿›åº¦: {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')

                        # ç»˜åˆ¶è®­ç»ƒæŸå¤±å›¾
                        fig_loss = go.Figure()
                        fig_loss.add_trace(go.Scatter(x=np.arange(1, epochs+1), y=train_losses, mode='lines+markers'))
                        fig_loss.update_layout(title='è®­ç»ƒæŸå¤±æ›²çº¿', xaxis_title='Epoch', yaxis_title='Loss')
                        st.plotly_chart(fig_loss, use_container_width=True)
                        status_text.text('è®­ç»ƒå®Œæˆ')
                        progress_bar.empty()

                        # ç”Ÿæˆé¢„æµ‹å€¼
                        model.eval()
                        with torch.no_grad():
                            # è®­ç»ƒé›†é¢„æµ‹
                            train_predictions = []
                            for batch_X, _ in train_loader:
                                batch_X = batch_X.to(device)
                                outputs = model(batch_X)
                                train_predictions.extend(outputs.cpu().numpy())
                            
                            # æµ‹è¯•é›†é¢„æµ‹
                            test_predictions = []
                            for batch_X, _ in test_loader:
                                batch_X = batch_X.to(device)
                                outputs = model(batch_X)
                                test_predictions.extend(outputs.cpu().numpy())
                            
                            # æœªæ¥é¢„æµ‹
                            future_X = generate_future_features(X, future_dates, feature_columns)
                            future_X_scaled = scaler_X.transform(future_X)
                            
                            # å‡†å¤‡æœ€åä¸€ä¸ªåºåˆ—ç”¨äºé¢„æµ‹
                            last_sequence = torch.FloatTensor(X_scaled[-sequence_length:]).unsqueeze(0).to(device)
                            future_predictions = []
                            
                            # é€æ­¥é¢„æµ‹æœªæ¥å€¼
                            for _ in range(len(future_dates)):
                                next_pred = model(last_sequence)
                                future_predictions.append(next_pred.item())
                                # æ›´æ–°åºåˆ—
                                last_sequence = torch.cat([
                                    last_sequence[:, 1:, :],
                                    torch.FloatTensor(future_X_scaled[_:_+1]).unsqueeze(0).to(device)
                                ], dim=1)
                            
                            # è½¬æ¢å›åŸå§‹æ¯”ä¾‹
                            y_train_pred = pd.Series(scaler_y.inverse_transform(np.array(train_predictions).reshape(-1, 1)).flatten(),index=train_index[sequence_length:])
                            y_test_pred = pd.Series(scaler_y.inverse_transform(np.array(test_predictions).reshape(-1, 1)).flatten(),index=test_index[sequence_length:])
                            future_pred = pd.Series(scaler_y.inverse_transform(np.array(future_predictions).reshape(-1, 1)).flatten(),index=future_dates)
                            
                            # è°ƒæ•´è¯„ä¼°æ•°æ®çš„èŒƒå›´
                            y_train = y_train.iloc[sequence_length:]
                            y_test = y_test.iloc[sequence_length:]
        
                            # ä½¿ç”¨ç»Ÿä¸€è¯„ä¼°å‡½æ•°è®¡ç®—æŒ‡æ ‡
                            train_metrics = evaluate_model(y_train, y_train_pred)
                            test_metrics = evaluate_model(y_test, y_test_pred)

                            # è°ƒç”¨ display_metrics å‡½æ•°æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
                            display_metrics(train_metrics, test_metrics)                

                    elif model_choice == "XGBoost":
                        import xgboost as xgb
                        model = xgb.XGBRegressor(
                            n_estimators=n_estimators,
                            max_depth=max_depth,
                            learning_rate=learning_rate,
                            subsample=subsample,
                            colsample_bytree=colsample_bytree,
                            reg_lambda=reg_lambda,
                            reg_alpha=reg_alpha
                        )
                        model.fit(X_train, y_train)
                        y_train_pred = pd.Series(model.predict(X_train), index=train_index)
                        y_test_pred = pd.Series(model.predict(X_test), index=test_index)
                        
                        shap_analysis(model, X_train, X_test, feature_columns)
                        # ä½¿ç”¨ç»Ÿä¸€è¯„ä¼°å‡½æ•°è®¡ç®—æŒ‡æ ‡
                        train_metrics = evaluate_model(y_train, y_train_pred)
                        test_metrics = evaluate_model(y_test, y_test_pred)

                        # è°ƒç”¨ display_metrics å‡½æ•°æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
                        display_metrics(train_metrics, test_metrics)
                        
                        # ç”Ÿæˆæœªæ¥é¢„æµ‹
                        future_X = generate_future_features(X, future_dates, feature_columns)
                        future_pred = pd.Series(model.predict(future_X), index=future_dates)

                    elif model_choice == "RandomForest":
                        model = RandomForestRegressor(
                            n_estimators=n_estimators,
                            max_depth=max_depth,
                            min_samples_split=min_samples_split,
                            min_samples_leaf=min_samples_leaf,
                            max_features=max_features
                        )
                        model.fit(X_train, y_train)
                        y_train_pred = pd.Series(model.predict(X_train), index=train_index)
                        y_test_pred = pd.Series(model.predict(X_test), index=test_index)

                        shap_analysis(model, X_train, X_test, feature_columns)
                        # ä½¿ç”¨ç»Ÿä¸€è¯„ä¼°å‡½æ•°è®¡ç®—æŒ‡æ ‡
                        train_metrics = evaluate_model(y_train, y_train_pred)
                        test_metrics = evaluate_model(y_test, y_test_pred)

                        # è°ƒç”¨ display_metrics å‡½æ•°æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
                        display_metrics(train_metrics, test_metrics)
                        
                        # ç”Ÿæˆæœªæ¥é¢„æµ‹
                        future_X = generate_future_features(X, future_dates, feature_columns)
                        future_pred = pd.Series(model.predict(future_X), index=future_dates)

                    elif model_choice == "LightGBM":
                        model = lgb.LGBMRegressor(
                            n_estimators=n_estimators,
                            max_depth=max_depth,
                            learning_rate=learning_rate,
                            num_leaves=num_leaves,
                            min_child_samples=min_child_samples,
                            metric=metric,
                            boosting_type=boosting_type
                        )
                        model.fit(X_train, y_train)
                        y_train_pred = pd.Series(model.predict(X_train), index=train_index)
                        y_test_pred = pd.Series(model.predict(X_test), index=test_index)

                        shap_analysis(model, X_train, X_test, feature_columns)
                        # ä½¿ç”¨ç»Ÿä¸€è¯„ä¼°å‡½æ•°è®¡ç®—æŒ‡æ ‡
                        train_metrics = evaluate_model(y_train, y_train_pred)
                        test_metrics = evaluate_model(y_test, y_test_pred)

                        # è°ƒç”¨ display_metrics å‡½æ•°æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
                        display_metrics(train_metrics, test_metrics)
                        
                        # ç”Ÿæˆæœªæ¥é¢„æµ‹
                        future_X = generate_future_features(X, future_dates, feature_columns)
                        future_pred = pd.Series(model.predict(future_X), index=future_dates)
                        
                    elif model_choice == "SVR":
                        model = SVR(kernel=kernel, C=C, epsilon=epsilon)
                        model.fit(X_train, y_train)
                        y_train_pred = pd.Series(model.predict(X_train), index=train_index)
                        y_test_pred = pd.Series(model.predict(X_test), index=test_index)

                        shap_analysis(model, X_train, X_test, feature_columns)                       
                        # ç”Ÿæˆæœªæ¥é¢„æµ‹
                        future_X = generate_future_features(X, future_dates, feature_columns)
                        future_pred = pd.Series(model.predict(future_X), index=future_dates)

                        # ä½¿ç”¨ç»Ÿä¸€è¯„ä¼°å‡½æ•°è®¡ç®—æŒ‡æ ‡
                        train_metrics = evaluate_model(y_train, y_train_pred)
                        test_metrics = evaluate_model(y_test, y_test_pred)

                        # è°ƒç”¨ display_metrics å‡½æ•°æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
                        display_metrics(train_metrics, test_metrics)
        
                    # åˆ›å»ºå•ä¸ªå›¾è¡¨æ›¿ä»£åŸæ¥çš„ä¸‰ä¸ªå­å›¾
                    fig = go.Figure()
        
                    # æ·»åŠ è®­ç»ƒé›†æ•°æ®
                    fig.add_trace(
                        go.Scatter(
                            x=y_train.index,
                            y=y_train.values,
                            name='è®­ç»ƒé›†å®é™…å€¼',
                            mode='lines',
                            line=dict(color='blue')
                        )
                    )
                    fig.add_trace(
                        go.Scatter(
                            x=y_train_pred.index,
                            y=y_train_pred.values,
                            name='è®­ç»ƒé›†é¢„æµ‹å€¼',
                            mode='lines',
                            line=dict(color='red', dash='dash')
                        )
                    )
        
                    # æ·»åŠ æµ‹è¯•é›†æ•°æ®
                    fig.add_trace(
                        go.Scatter(
                            x=y_test.index,
                            y=y_test.values,
                            name='æµ‹è¯•é›†å®é™…å€¼',
                            mode='lines',
                            line=dict(color='green')
                        )
                    )
                    fig.add_trace(
                        go.Scatter(
                            x=y_test_pred.index,
                            y=y_test_pred.values,
                            name='æµ‹è¯•é›†é¢„æµ‹å€¼',
                            mode='lines',
                            line=dict(color='orange', dash='dash')
                        )
                    )
        
                    # æ·»åŠ æœªæ¥é¢„æµ‹æ•°æ®
                    fig.add_trace(
                        go.Scatter(
                            x=future_dates,
                            y=future_pred.values,
                            name='æœªæ¥é¢„æµ‹',
                            mode='lines',
                            line=dict(color='purple')
                        )
                    )
        
                    # æ›´æ–°å¸ƒå±€
                    fig.update_layout(
                        height=600,
                        title=f"{model_choice}æ¨¡å‹é¢„æµ‹ç»“æœ",
                        xaxis_title="æ—¶é—´",
                        yaxis_title=target_column,
                        showlegend=True,
                        legend=dict(
                            orientation="h",
                            yanchor="bottom",
                            y=1.02,
                            xanchor="right",
                            x=1
                        ),
                        # æ·»åŠ å‚ç›´åˆ†éš”çº¿æ¥åŒºåˆ†è®­ç»ƒé›†ã€æµ‹è¯•é›†å’Œæœªæ¥é¢„æµ‹
                        shapes=[
                            # è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†éš”çº¿
                            dict(
                                type="line",
                                x0=test_index[0],
                                x1=test_index[0],
                                y0=0,
                                y1=1,
                                yref="paper",
                                line=dict(color="gray", width=1, dash="dash")
                            ),
                            # æµ‹è¯•é›†å’Œæœªæ¥é¢„æµ‹çš„åˆ†éš”çº¿
                            dict(
                                type="line",
                                x0=future_dates[0],
                                x1=future_dates[0],
                                y0=0,
                                y1=1,
                                yref="paper",
                                line=dict(color="gray", width=1, dash="dash")
                            )
                        ],
                        # æ·»åŠ æ³¨é‡Š
                        annotations=[
                            dict(
                                x=y_train.index[len(y_train)//2],
                                y=1.05,
                                yref="paper",
                                text="è®­ç»ƒé›†",
                                showarrow=False
                            ),
                            dict(
                                x=y_test.index[len(y_test)//2],
                                y=1.05,
                                yref="paper",
                                text="æµ‹è¯•é›†",
                                showarrow=False
                            ),
                            dict(
                                x=future_dates[len(future_dates)//2],
                                y=1.05,
                                yref="paper",
                                text="æœªæ¥é¢„æµ‹",
                                showarrow=False
                            )
                        ]
                    )
        
                    # æ˜¾ç¤ºå›¾è¡¨
                    st.plotly_chart(fig, use_container_width=True)
        
                except Exception as e:
                    st.error(f"æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
                    st.warning("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œé€‰æ‹©çš„ç‰¹å¾æ˜¯å¦åˆé€‚ã€‚")
                    

# åœ¨ä¸»è¦ä»£ç çš„æœ€åæ·»åŠ ä¾§è¾¹æ ä¿¡æ¯

st.sidebar.markdown("---")

# æ·»åŠ ç‰ˆæœ¬è®°å½•
with st.sidebar.expander("ç‰ˆæœ¬è®°å½•", expanded=True, icon="ğŸš¨"):
    st.markdown("""
    **1.1** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ æ—¶é—´åºåˆ—åˆ†è§£åˆ†ææ¨¡å—ã€‚  
    **1.2** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ é¢‘è°±åˆ†æçš„æ•°æ®ç­›é€‰é€‰é¡¹ã€‚  
    **1.3** æ”¹ç‰ˆå†…å®¹ï¼šå¤šæ–‡æ¡£ä¸Šä¼ åçš„æ—¥æœŸä¿¡æ¯æå–ï¼›ä¼˜åŒ–çº¿æ€§æ’å€¼åçš„ç©ºå€¼å¤„ç†ã€‚  
    **1.4** æ”¹ç‰ˆå†…å®¹ï¼šæ¨¡å‹è®­ç»ƒæ¨¡å—æ·»åŠ äº†æœªæ¥é¢„æµ‹å¤©æ•°çš„é€‰æ‹©ã€‚  
    **1.5** æ”¹ç‰ˆå†…å®¹ï¼šéƒ¨åˆ†å›¾è¡¨ä½¿ç”¨plotlyä¼˜åŒ–æ›²çº¿æ˜¾ç¤ºæ•ˆæœã€‚  
    **1.6** æ”¹ç‰ˆå†…å®¹ï¼šæ¨¡å‹è®­ç»ƒå¢åŠ LSTMç®—æ³•ã€‚  
    **1.7** æ”¹ç‰ˆå†…å®¹ï¼šä»£ç ç»“æ„åŒ–ï¼Œä¾¿äºç»´æŠ¤ã€‚  
    **1.8** æ”¹ç‰ˆå†…å®¹ï¼šä»£ç å›é€€è‡³V1.6ç‰ˆæœ¬ï¼Œå¹¶è§£å†³æ·»åŠ äº†æ—¶åºæ•°æ®è·¨é›¶ç‚¹æ£€æµ‹é€»è¾‘ï¼ˆå½“æ£€æµ‹åˆ°è·¨é›¶ç‚¹æ—¶ï¼Œè‡ªåŠ¨æ·»åŠ æ—¥æœŸåç§»ï¼‰ã€‚åŒæ—¶ä¼˜åŒ–äº†æ¨¡å‹è®­ç»ƒåçš„å¯è§†åŒ–æ˜¾ç¤ºã€‚  
    **1.9** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ XGBoostæ¨¡å‹è®­ç»ƒç®—æ³•ã€‚  
    **2.0** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ RandomForestæ¨¡å‹è®­ç»ƒç®—æ³•ã€‚  
    **2.1** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ LightGBMæ¨¡å‹è®­ç»ƒç®—æ³•ã€‚  
    **2.2** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ SVRæ¨¡å‹è®­ç»ƒç®—æ³•ã€‚  
    **2.3** æ”¹ç‰ˆå†…å®¹ï¼šæ·»åŠ ç»Ÿä¸€çš„æ¨¡å‹è¯„ä¼°å‡½æ•°ä½¿æ‰€æœ‰æ¨¡å‹ï¼ˆLSTM/XGBoost/RandomForest/LightGBM/SVRï¼‰éƒ½æœ‰ç›¸åŒçš„è¯„ä¼°æŒ‡æ ‡ã€‚  
    **2.4** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼Œä¼˜åŒ–ç›¸å…³æ€§çƒ­å›¾æ˜¾ç¤ºã€‚   
    **2.5** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ LSTMæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®­ç»ƒæŸå¤±æ›²çº¿æ˜¾ç¤ºã€‚   
    **2.6** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ GRUæ¨¡å‹è®­ç»ƒç®—æ³•å’Œå½’ä¸€åŒ–æ–¹æ³•ï¼ˆæœ€å¤§ç»å¯¹å€¼å½’ä¸€åŒ–ã€Robust Scalerå’ŒL2å½’ä¸€åŒ–ï¼‰ã€‚   
    **2.7** æ”¹ç‰ˆå†…å®¹ï¼šå¢åŠ SHAPå€¼åˆ†æåŠŸèƒ½ï¼Œä¼˜åŒ–è¯„ä¼°æŒ‡æ ‡æ˜¾ç¤ºã€‚   
    """)
        
st.sidebar.markdown("---")
        
# æ·»åŠ å¼€å‘è€…ä¿¡æ¯
st.sidebar.markdown("""
### å¼€å‘è€…ä¿¡æ¯
        
**å¼€å‘è€…**ï¼šç‹åº·ä¸š  
**é‚®ç®±**ï¼škangy_wang@hnair.com
        
---
        
### ç‰ˆæƒå£°æ˜
        
Copyright Â© 2025 ç‹åº·ä¸š. All Rights Reserved.  
                            
æœ¬åº”ç”¨ç¨‹åºå—è‘—ä½œæƒæ³•å’Œå…¶ä»–çŸ¥è¯†äº§æƒæ³•ä¿æŠ¤ã€‚  
æœªç»æˆæƒï¼Œç¦æ­¢å¤åˆ¶ã€ä¿®æ”¹æˆ–åˆ†å‘æœ¬ç¨‹åºçš„ä»»ä½•éƒ¨åˆ†ã€‚
                    
Version 2.7.0
""")
        
# æ·»åŠ ä¸€äº›ç©ºè¡Œæ¥ç¡®ä¿ç‰ˆæƒä¿¡æ¯åœ¨åº•éƒ¨
st.sidebar.markdown("<br>" * 5, unsafe_allow_html=True)
